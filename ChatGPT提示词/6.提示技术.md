> 时至今日，改进提示显然有助于在不同任务上获得更好的结果。这就是提示工程背后的整个理念。
>
> 尽管基础示例很有趣，但在本节中，我们将介绍更高级的提示工程技术，使我们能够完成更复杂和有趣的任务。

### 零样本提示

如今，经过大量数据训练并调整指令的LLM能够执行零样本任务。我们在前一节中尝试了一些零样本示例。以下是我们使用的一个示例：

*提示：*

```Plaintext
将文本分类为中性、负面或正面。文本：我认为这次假期还可以。情感：
```

*输出：*

```Plaintext
中性
```

请注意，在上面的提示中，我们没有向模型提供任何示例——这就是零样本能力的作用。

指令调整已被证明可以改善零样本学习[Wei等人（2022）(opens in a new tab)](https://arxiv.org/pdf/2109.01652.pdf)。指令调整本质上是在通过指令描述的数据集上微调模型的概念。此外，[RLHF(opens in a new tab)](https://arxiv.org/abs/1706.03741)（来自人类反馈的强化学习）已被采用以扩展指令调整，其中模型被调整以更好地适应人类偏好。这一最新发展推动了像ChatGPT这样的模型。我们将在接下来的章节中讨论所有这些方法和方法。

当零样本不起作用时，建议在提示中提供演示或示例，这将导致少样本提示。在下一节中，我们将演示少样本提示。

### 少样本提示

虽然大型语言模型展示了惊人的零样本能力，但在使用零样本设置时，它们在更复杂的任务上仍然表现不佳。少样本提示可以作为一种技术，以启用上下文学习，我们在提示中提供演示以引导模型实现更好的性能。演示作为后续示例的条件，我们希望模型生成响应。

让我们通过[Brown等人2020年(opens in a new tab)](https://arxiv.org/abs/2005.14165)提出的一个例子来演示少样本提示。在这个例子中，任务是在句子中正确使用一个新词。

*提示：*

```Plaintext
“whatpu”是坦桑尼亚的一种小型毛茸茸的动物。一个使用whatpu这个词的句子的例子是：我们在非洲旅行时看到了这些非常可爱的whatpus。“farduddle”是指快速跳上跳下。一个使用farduddle这个词的句子的例子是：
```

*输出：*

```Plaintext
当我们赢得比赛时，我们都开始庆祝跳跃。
```

我们可以观察到，模型通过提供一个示例（即1-shot）已经学会了如何执行任务。对于更困难的任务，我们可以尝试增加演示（例如3-shot、5-shot、10-shot等）。

根据[Min等人（2022）(opens in a new tab)](https://arxiv.org/abs/2202.12837)的研究结果，以下是在进行少样本学习时关于演示/范例的一些额外提示：

- “标签空间和演示指定的输入文本的分布都很重要（无论标签是否对单个输入正确）”
- 使用的格式也对性能起着关键作用，即使只是使用随机标签，这也比没有标签好得多。
- 其他结果表明，从真实标签分布（而不是均匀分布）中选择随机标签也有帮助。

让我们尝试一些例子。让我们首先尝试一个随机标签的例子（意味着将标签Negative和Positive随机分配给输入）：

*提示：*

```Plaintext
这太棒了！// Negative这太糟糕了！// Positive哇，那部电影太棒了！// Positive多么可怕的节目！//
```

*输出：*

```Plaintext
Negative
```

即使标签已经随机化，我们仍然得到了正确的答案。请注意，我们还保留了格式，这也有助于。实际上，通过进一步的实验，我们发现我们正在尝试的新GPT模型甚至对随机格式也变得更加稳健。例如：

*提示：*

```Plaintext
Positive This is awesome! This is bad! NegativeWow that movie was rad!PositiveWhat a horrible show! --
```

*输出：*

```Plaintext
Negative
```

上面的格式不一致，但模型仍然预测了正确的标签。我们必须进行更彻底的分析，以确认这是否适用于不同和更复杂的任务，包括提示的不同变体。

#### 少样本提示的限制

标准的少样本提示对许多任务都有效，但仍然不是一种完美的技术，特别是在处理更复杂的推理任务时。让我们演示为什么会这样。您是否还记得之前提供的任务：

```Plaintext
这组数字中的奇数加起来是一个偶数：15、32、5、13、82、7、1。A：
```

如果我们再试一次，模型输出如下：

```Plaintext
是的，这组数字中的奇数加起来是107，是一个偶数。
```

这不是正确的答案，这不仅突显了这些系统的局限性，而且需要更高级的提示工程。

让我们尝试添加一些示例，看看少样本提示是否可以改善结果。

*提示：*

```Plaintext
这组数字中的奇数加起来是一个偶数：4、8、9、15、12、2、1。A：答案是False。这组数字中的奇数加起来是一个偶数：17、10、19、4、8、12、24。A：答案是True。这组数字中的奇数加起来是一个偶数：16、11、14、4、8、13、24。A：答案是True。这组数字中的奇数加起来是一个偶数：17、9、10、12、13、4、2。A：答案是False。这组数字中的奇数加起来是一个偶数：15、32、5、13、82、7、1。A：
```

*输出：*

```Plaintext
答案是True。
```

这没用。似乎少样本提示不足以获得这种类型的推理问题的可靠响应。上面的示例提供了任务的基本信息。如果您仔细观察，我们引入的任务类型涉及几个更多的推理步骤。换句话说，如果我们将问题分解成步骤并向模型演示，这可能会有所帮助。最近，[思维链（CoT）提示(opens in a new tab)](https://arxiv.org/abs/2201.11903)已经流行起来，以解决更复杂的算术、常识和符号推理任务。

总的来说，提供示例对解决某些任务很有用。当零样本提示和少样本提示不足时，这可能意味着模型学到的东西不足以在任务上表现良好。从这里开始，建议开始考虑微调您的模型或尝试更高级的提示技术。接下来，我们将讨论一种流行的提示技术，称为思维链提示，它已经获得了很多关注。

### 链式思考（CoT）提示

#### 链式思考（CoT）提示

![img](images/(null)-20230514233628708.(null))

图片来源：[Wei等人（2022）(opens in a new tab)](https://arxiv.org/abs/2201.11903)

在[Wei等人（2022）(opens in a new tab)](https://arxiv.org/abs/2201.11903)中引入的链式思考（CoT）提示通过中间推理步骤实现了复杂的推理能力。您可以将其与少样本提示相结合，以获得更好的结果，以便在回答之前进行推理的更复杂的任务。

*提示：*

```Plaintext
这组数中的奇数加起来是偶数：4、8、9、15、12、2、1。A：将所有奇数相加（9、15、1）得到25。答案为False。这组数中的奇数加起来是偶数：17、10、19、4、8、12、24。A：将所有奇数相加（17、19）得到36。答案为True。这组数中的奇数加起来是偶数：16、11、14、4、8、13、24。A：将所有奇数相加（11、13）得到24。答案为True。这组数中的奇数加起来是偶数：17、9、10、12、13、4、2。A：将所有奇数相加（17、9、13）得到39。答案为False。这组数中的奇数加起来是偶数：15、32、5、13、82、7、1。A：
```

*输出：*

```Plaintext
将所有奇数相加（15、5、13、7、1）得到41。答案为False。
```

哇！我们可以看到在提供推理步骤时得到了完美的结果。实际上，我们可以通过提供更少的示例来解决此任务，即仅一个示例似乎就足够了：

*提示：*

```Plaintext
这组数中的奇数加起来是偶数：4、8、9、15、12、2、1。A：将所有奇数相加（9、15、1）得到25。答案为False。这组数中的奇数加起来是偶数：15、32、5、13、82、7、1。A：
```

*输出：*

```Plaintext
将所有奇数相加（15、5、13、7、1）得到41。答案为False。
```

请记住，作者声称这是足够大的语言模型才会出现的新兴能力。

#### 零样本COT提示

![img](images/(null)-20230514233628002.(null))

图片来源：[Kojima等人（2022）(opens in a new tab)](https://arxiv.org/abs/2205.11916)

最近提出的一个新想法是[零样本CoT(opens in a new tab)](https://arxiv.org/abs/2205.11916)（Kojima等人，2022年），它基本上涉及将“让我们逐步思考”添加到原始提示中。让我们尝试一个简单的问题，看看模型的表现如何：

*提示：*

```Plaintext
我去市场买了10个苹果。我给了邻居2个苹果和修理工2个苹果。然后我去买了5个苹果并吃了1个。我还剩下多少苹果？
```

*输出：*

```Plaintext
11个苹果
```

答案是不正确的！现在让我们尝试使用特殊提示。

*提示：*

```Plaintext
我去市场买了10个苹果。我给了邻居2个苹果和修理工2个苹果。然后我去买了5个苹果并吃了1个。我还剩下多少苹果？让我们逐步思考。
```

*输出：*

```Plaintext
首先，您从10个苹果开始。您给了邻居和修理工各2个苹果，所以您还剩下6个苹果。然后您买了5个苹果，所以现在您有11个苹果。最后，您吃了1个苹果，所以您还剩下10个苹果。
```

令人印象深刻的是，这个简单的提示在这个任务中非常有效。这在您没有太多示例可用于提示时特别有用。

### 自我一致性

也许在提示工程中更高级的技术之一是自我一致性。由[Wang等人（2022）(opens in a new tab)](https://arxiv.org/pdf/2203.11171.pdf)提出，自我一致性旨在“替换链式思维提示中使用的天真贪婪解码方法”。其想法是通过少样本CoT采样多个不同的推理路径，并使用生成结果选择最一致的答案。这有助于提高CoT提示在涉及算术和常识推理的任务中的性能。

让我们尝试以下算术推理示例：

*提示：*

```Plaintext
当我6岁时，我的妹妹是我的一半年龄。现在我70岁了，我的妹妹多大？
```

*输出：*

```Plaintext
35
```

输出是错误的！我们如何通过自我一致性来改进这个问题？让我们试试。我们将使用Wang等人2022年的少量样本范例（表17）：

*提示：*

```Plaintext
Q：林中有15棵树。林业工人今天将在林中种树。完成后，将有21棵树。林业工人今天种了多少棵树？
A：我们从15棵树开始。后来我们有21棵树。差异必须是他们种树的数量。因此，他们必须种了21-15 = 6棵树。答案是6。

Q：停车场有3辆汽车，又来了2辆汽车，停车场有多少辆汽车？A：停车场已经有3辆汽车。又来了2辆。现在有3 + 2 = 5辆汽车。答案是5。Q：Leah有32块巧克力，她的姐姐有42块。如果他们吃了35块，他们总共还剩多少块？
A：Leah有32块巧克力，Leah的姐姐有42块。这意味着最初有32 + 42 = 74块巧克力。已经吃了35块。因此，他们总共还剩74-35 = 39块巧克力。答案是39。

Q：Jason有20个棒棒糖。他给Denny一些棒棒糖。现在Jason只有12个棒棒糖。Jason给Denny多少棒棒糖？
A：Jason有20个棒棒糖。因为他现在只有12个，所以他必须把剩下的给Denny。他给Denny的棒棒糖数量必须是20-12 = 8个棒棒糖。答案是8。

Q：Shawn有五个玩具。圣诞节，他从他的父母那里得到了两个玩具。他现在有多少个玩具？
A：他有5个玩具。他从妈妈那里得到了2个，所以在那之后他有5 + 2 = 7个玩具。然后他从爸爸那里得到了2个，所以总共他有7 + 2 = 9个玩具。答案是9。

Q：服务器房间里有9台计算机。从周一到周四，每天都会安装5台计算机。现在服务器房间里有多少台计算机？
A：从周一到周四有4天。每天都添加了5台计算机。这意味着总共添加了4 * 5 =20台计算机。一开始有9台计算机，所以现在有9 + 20 = 29台计算机。答案是29。

Q：Michael有58个高尔夫球。星期二，他丢失了23个高尔夫球。星期三，他又丢失了2个。星期三结束时他还剩多少个高尔夫球？
A：Michael最初有58个球。星期二他丢失了23个，所以在那之后他有58-23 = 35个球。星期三他又丢失了2个，所以现在他有35-2 = 33个球。答案是33。

Q：Olivia有23美元。她用每个3美元的价格买了五个百吉饼。她还剩多少钱？
A：她用每个3美元的价格买了5个百吉饼。这意味着她花了5Q：当我6岁时，我的妹妹是我的一半年龄。现在我70岁了，我的妹妹多大？
A：
```

*输出1：*

```Plaintext
当我6岁时，我的妹妹是我的一半年龄，也就是3岁。现在我70岁了，所以她是70-3 = 67岁。答案是67。
```

*输出2：*

```Plaintext
当叙述者6岁时，他的妹妹是他年龄的一半，也就是3岁。现在叙述者70岁了，他的妹妹应该是70-3 = 67岁。答案是67。
```

*输出3：*

```Plaintext
当我6岁时，我的妹妹是我的一半年龄，也就是3岁。现在我70岁了，所以她是70/2 = 35岁。答案是35。
```

计算最终答案涉及几个步骤（详见论文），但为了简单起见，我们可以看到已经出现了大多数答案，因此这基本上将成为最终答案。

### 生成知识提示

![img](images/(null)-20230514233904450.(null))

图片来源：[Liu等人2022(opens in a new tab)](https://arxiv.org/pdf/2110.08387.pdf)

LLM继续得到改进，其中一种流行的技术是能够融合知识或信息，以帮助模型做出更准确的预测。

使用类似的思路，模型是否也可以在做出预测之前用于生成知识呢？这就是[Liu等人2022(opens in a new tab)](https://arxiv.org/pdf/2110.08387.pdf)的论文所尝试的——生成知识以作为提示的一部分。特别是，这对于常识推理等任务有多大帮助？

让我们尝试一个简单的提示：

*提示：*

```Plaintext
高尔夫球的一部分是试图获得比其他人更高的得分。是或否？
```

*输出：*

```Plaintext
是。
```

这种错误揭示了LLM在执行需要更多关于世界的知识的任务时的局限性。我们如何通过生成知识来改进呢？

首先，我们生成一些“知识”：

*提示：*

```Plaintext
输入：希腊比墨西哥大。知识：希腊的面积约为131,957平方公里，而墨西哥的面积约为1,964,375平方公里，使墨西哥比希腊大了1,389%。
输入：眼镜总是会起雾。知识：当你的汗水、呼吸和周围的湿度中的水蒸气落在冷的表面上，冷却并变成微小的液滴时，会在眼镜镜片上产生冷凝。你看到的是一层薄膜。你的镜片相对于你的呼吸会比较凉，尤其是当外面的空气很冷时。
输入：鱼有思考能力。知识：鱼比它们看起来更聪明。在许多领域，如记忆力，它们的认知能力与或超过非人类灵长类动物等“更高级”的脊椎动物。鱼的长期记忆帮助它们跟踪复杂的社交关系。
输入：一个人一生中吸烟很多香烟的常见影响是患肺癌的几率高于正常水平。知识：那些一生中平均每天吸烟不到一支香烟的人，患肺癌的风险是从不吸烟者的9倍。在每天吸烟1到10支香烟之间的人群中，死于肺癌的风险几乎是从不吸烟者的12倍。
输入：一块石头和一颗卵石大小相同。知识：卵石是一种根据Udden-Wentworth沉积学尺度的颗粒大小为4到64毫米的岩屑。卵石通常被认为比颗粒（直径2到4毫米）大，比卵石（直径64到256毫米）小。
输入：高尔夫球的一部分是试图获得比其他人更高的得分。知识：
```

*q1：*

```Plaintext
高尔夫球的目标是以最少的杆数打完一组洞。一轮高尔夫球比赛通常包括18个洞。每个洞在标准高尔夫球场上一轮只打一次。每个杆计为一分，总杆数用于确定比赛的获胜者。
```

*q2：*

```Plaintext
高尔夫球是一项精密的球杆运动，竞争选手（或高尔夫球手）使用许多类型的球杆，用最少的杆数将球打入球场上一系列洞中。目标是以最低的分数完成比赛，该分数是通过将每个洞上所用的杆数相加计算出来的。得分最低的选手赢得比赛。
```

我们使用[Liu等人2022(opens in a new tab)](https://arxiv.org/pdf/2110.08387.pdf)的论文中提供的提示。

下一步是将知识整合并得出预测。我将问题重新格式化为QA格式，以指导答案格式。

*提示：*

```Plaintext
问题：高尔夫球的一部分是试图获得比其他人更高的得分。是或否？知识：高尔夫球的目标是以最少的杆数打完一组洞。一轮高尔夫球比赛通常包括18个洞。每个洞在标准高尔夫球场上一轮只打一次。每个杆计为一分，总杆数用于确定比赛的获胜者。解释和答案： 
```

*答案1（置信度非常高）：*

```Plaintext
不是，高尔夫球的目标不是获得比其他人更高的得分。相反，目标是以最少的杆数打完一组洞。总杆数用于确定比赛的获胜者，而不是总得分。
问题：高尔夫球的一部分是试图获得比其他人更高的得分。是或否？知识：高尔夫球是一项精密的球杆运动，竞争选手（或高尔夫球手）使用许多类型的球杆，用最少的杆数将球打入球场上一系列洞中。目标是以最低的分数完成比赛，该分数是通过将每个洞上所用的杆数相加计算出来的。得分最低的选手赢得比赛。解释和答案：
```

*答案2（置信度要低得多）：*

```Plaintext
是的，高尔夫球的一部分是试图获得比其他人更高的得分。每个选手都试图以最少的杆数打完一组洞。总杆数用于确定比赛的获胜者，而不是总得分。得分最低的选手赢得比赛。
```

这个例子发生了一些非常有趣的事情。在第一个答案中，模型非常自信，但在第二个答案中不太自信。我简化了过程以进行演示，但在得出最终答案时还有一些细节需要考虑。请查看论文以了解更多。

### 自动提示工程师（APE）

[Zhou等人，（2022）(opens in a new tab)](https://arxiv.org/abs/2211.01910) 提出了自动提示工程师（APE），这是一个用于自动指令生成和选择的框架。指令生成问题被构建为自然语言合成问题，使用LLMs作为黑盒优化问题的解决方案来生成和搜索候选解。

第一步涉及一个大型语言模型（作为推理模型），该模型接收输出演示以生成任务的指令候选项。这些候选解将指导搜索过程。使用目标模型执行指令，然后根据计算的评估分数选择最合适的指令。

APE发现了一个比人工设计的“让我们一步一步地思考”提示更好的零样本CoT提示（[Kojima等人，2022(opens in a new tab)](https://arxiv.org/abs/2205.11916)）。

提示“让我们一步一步地解决这个问题，以确保我们有正确的答案。”引发了思维链的推理，并提高了MultiArith和GSM8K基准测试的性能：

![img](images/(null)-20230514234059119.(null))

图片来源：[Zhou等人，（2022）(opens in a new tab)](https://arxiv.org/abs/2211.01910)

本文涉及与提示工程相关的重要主题，即自动优化提示的想法。虽然我们在本指南中没有深入探讨这个主题，但如果您对此主题感兴趣，以下是一些关键论文：

- [AutoPrompt(opens in a new tab)](https://arxiv.org/abs/2010.15980) - 提出了一种基于梯度引导搜索的方法，用于自动创建各种任务的提示。
- [Prefix Tuning(opens in a new tab)](https://arxiv.org/abs/2101.00190) - 是一种轻量级的fine-tuning替代方案，为NLG任务添加可训练的连续前缀。
- [Prompt Tuning(opens in a new tab)](https://arxiv.org/abs/2104.08691) - 提出了一种通过反向传播学习软提示的机制。

### Active-Prompt

思维链（CoT）方法依赖于一组固定的人工注释范例。问题在于，这些范例可能不是不同任务的最有效示例。为了解决这个问题，[Diao等人（2023）(opens in a new tab)](https://arxiv.org/pdf/2302.12246.pdf)最近提出了一种新的提示方法，称为Active-Prompt，以适应LLMs到不同的任务特定示例提示（用人类设计的CoT推理进行注释）。

下面是该方法的说明。第一步是使用或不使用少量CoT示例查询LLM。对一组训练问题生成*k*个可能的答案。基于*k*个答案计算不确定度度量（使用不一致性）。选择最不确定的问题由人类进行注释。然后使用新的注释范例来推断每个问题。

![img](images/(null)-20230514234116487.(null))

图片来源：[Diao等人（2023）](https://arxiv.org/pdf/2302.12246.pdf)

### 方向性刺激提示

[Li等人，（2023）(opens in a new tab)](https://arxiv.org/abs/2302.11520)提出了一种新的提示技术，以更好地指导LLM生成所需的摘要。

训练了一个可调节的策略LM来生成刺激/提示。越来越多地使用RL来优化LLM。

下图显示了方向性刺激提示与标准提示的比较。策略LM可以很小，并且可以优化以生成指导黑盒冻结LLM的提示。

![img](images/(null)-20230514234135605.(null))

图片来源：[Li等人，（2023）(opens in a new tab)](https://arxiv.org/abs/2302.11520)

完整示例即将推出！

### ReAct

从[Yao等人，2022(opens in a new tab)](https://arxiv.org/abs/2210.03629)引入了一个框架，其中LLMs以交错的方式生成推理轨迹和任务特定的操作。生成推理轨迹使模型能够诱导、跟踪和更新行动计划，甚至处理异常情况。操作步骤允许与外部源（如知识库或环境）进行接口和信息收集。

ReAct框架可以使LLMs与外部工具交互，以检索导致更可靠和事实的响应的附加信息。

![img](images/(null)-20230514234154594.(null))

图片来源：[Yao等人，2022(opens in a new tab)](https://arxiv.org/abs/2210.03629)

完整的示例即将推出！

### 多模态思维链提示方法

最近，[Zhang等人（2023）(opens in a new tab)](https://arxiv.org/abs/2302.00923)提出了一种多模态思维链提示方法。传统的思维链提示方法侧重于语言模态。相比之下，多模态思维链提示将文本和视觉融入到一个两阶段框架中。第一步涉及基于多模态信息的理性生成。接下来是第二阶段的答案推断，它利用生成的理性信息。

多模态CoT模型（1B）在ScienceQA基准测试中的表现优于GPT-3.5。

![img](images/(null)-20230514234214023.(null))

图片来源：[Zhang et al. (2023)(opens in a new tab)](https://arxiv.org/abs/2302.00923)

进一步阅读：

- [语言不是你所需要的全部：将感知与语言模型对齐(opens in a new tab)](https://arxiv.org/abs/2302.14045)（2023年2月）